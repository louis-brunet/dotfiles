-- ---@type AvanteProvider[]
-- local avante_vendors = {
--     ---@class CustomOllamaAvanteProvider: AvanteProvider
--     ollama = {
--         ["local"] = true,
--         use_xml_format = true,
--         endpoint = "127.0.0.1:11434/api/chat",
--         model = "llama3.2:1b",
--         -- model = "llama3.2:3b",
--         -- model = "codegemma",
--         -- model = "deepseek-coder:6.7b",
--
--         max_tokens = 8192,
--         stream = true,
--         keep_alive = '10m',
--         num_ctx = 8192,
--         temperature = 0.1,
--         top_p = 0.9,
--         top_k = 40,
--
--         ---@param opts CustomOllamaAvanteProvider
--         ---@param code_opts AvantePromptOptions
--         ---@return AvanteCurlOutput
--         parse_curl_args = function(opts, code_opts)
--             local messages = {
--                 { role = "system", content = code_opts.system_prompt },
--                 { role = "user",   content = require("avante.providers.openai").get_user_message(code_opts) },
--             }
--             -- vim.notify("[parse_curl_args] messages="..vim.inspect(messages));
--
--             return {
--                 url = opts.endpoint,
--                 headers = {
--                     ["Accept"] = "application/json",
--                     ["Content-Type"] = "application/json",
--                 },
--                 body = {
--                     model = opts.model,
--                     messages = messages,
--                     stream = opts.stream,
--                     keep_alive = opts.keep_alive,
--                     options = {
--                         num_predict = opts.max_tokens,
--                         num_ctx = opts.num_ctx,
--                         temperature = opts.temperature,
--                         top_k = opts.top_k,
--                         top_p = opts.top_p,
--                     },
--                 },
--             }
--         end,
--
--         -- parse_response_data = function(data_stream, _, opts)
--         parse_stream_data = function(line, handler_opts)
--             -- require("avante.providers").openai.parse_response(data_stream, event_state, opts)
--
--             ---@class OllamaChatResponseMessage
--             ---@field role "assistant"
--             ---@field content string
--             ---@field image? string
--
--             ---@class OllamaChatResponse
--             ---@field model string
--             ---@field created_at string
--             ---@field done boolean
--             ---@field message? OllamaChatResponseMessage
--
--             -- print("data_stream: " .. vim.inspect(data_stream))
--             -- vim.notify("data_stream: " .. vim.inspect(data_stream))
--
--             -- if data_stream:match('"%[DONE%]":') then
--             --     opts.on_complete(nil)
--             --     return
--             -- end
--             -- if data_stream:match('"done":') then
--             ---@type OllamaChatResponse
--             local json = vim.json.decode(line)
--             if json.message and json.message.content ~= nil then
--                 handler_opts.on_chunk(json.message.content)
--             end
--             if json.done then
--                 handler_opts.on_complete(nil)
--                 return
--             end
--             -- end
--         end,
--     },
-- }

---@type LazySpec
local M = {
    -- {
    --     "yetone/avante.nvim",
    --     event = "VeryLazy",
    --     lazy = false,
    --     version = false, -- set this if you want to always pull the latest change
    --     ---@type avante.Config
    --     opts = {
    --         provider = "ollama",
    --         vendors = avante_vendors,
    --         -- add any opts here
    --     },
    --     -- if you want to build from source then do `make BUILD_FROM_SOURCE=true`
    --     build = "make",
    --     -- build = "powershell -ExecutionPolicy Bypass -File Build.ps1 -BuildFromSource false" -- for windows
    --     dependencies = {
    --         "nvim-treesitter/nvim-treesitter",
    --         "stevearc/dressing.nvim",
    --         "nvim-lua/plenary.nvim",
    --         "MunifTanjim/nui.nvim",
    --         --- The below dependencies are optional,
    --         "nvim-tree/nvim-web-devicons", -- or echasnovski/mini.icons
    --         -- "zbirenbaum/copilot.lua", -- for providers='copilot'
    --         {
    --             -- support for image pasting
    --             "HakonHarnes/img-clip.nvim",
    --             event = "VeryLazy",
    --             opts = {
    --                 -- recommended settings
    --                 default = {
    --                     embed_image_as_base64 = false,
    --                     prompt_for_file_name = false,
    --                     drag_and_drop = {
    --                         insert_mode = true,
    --                     },
    --                     -- required for Windows users
    --                     use_absolute_path = true,
    --                 },
    --             },
    --         },
    --         {
    --             -- Make sure to set this up properly if you have lazy=true
    --             'MeanderingProgrammer/render-markdown.nvim',
    --             opts = {
    --                 file_types = { "markdown", "Avante" },
    --             },
    --             ft = { "markdown", "Avante" },
    --         },
    --     },
    -- }

}
return M
